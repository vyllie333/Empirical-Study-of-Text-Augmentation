{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gKd9bo2pJX5f"},"outputs":[],"source":["pip install pyvi"]},{"cell_type":"markdown","metadata":{"id":"BmlhP5t39Khb"},"source":["### EDA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1671944739020,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"},"user_tz":-420},"id":"KE3IrHNYK99b","outputId":"b45913be-5856-459a-f57f-2252448a1410"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'vietnamese-stopwords' already exists and is not an empty directory.\n"]}],"source":["#Stopword Tiếng Việt\n","!git clone https://github.com/stopwords/vietnamese-stopwords.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Vu4x4ytLjbY"},"outputs":[],"source":["!git clone https://github.com/sonlam1102/text_augmentation_vietnamese.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eiUrkcvF0YFu"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojG2qAfm1CAC"},"outputs":[],"source":["FD_PATH = '/content/demo_aug.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IAqENq1KnnyB"},"outputs":[],"source":["# Xem sơ qua bộ dữ liệu\n","import pandas as pd\n","df = pd.read_csv(FD_PATH, encoding = 'utf-8',header=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C2xQkXwzqDPd"},"outputs":[],"source":["lines = open(FD_PATH, 'r', encoding=\"utf-8\").readlines()\n","lines"]},{"cell_type":"markdown","source":["### Data Augmentaion"],"metadata":{"id":"Gl-wg2ZQb5Fu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EBQYcttrgV1"},"outputs":[],"source":["# Change these arguments to fit with your own data / project\n","\n","class Argument:\n","    input = FD_PATH   # file data input .csv \n","    output = \"/content/aug.txt\"     # file data output .txt\n","    num_aug = 8                     # 1 câu góc tạo ra 8 câu\n","    alpha = 0.1                  \n","\n","\n","args = Argument()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qkajTdlgKzv3"},"outputs":[],"source":["import random\n","from random import shuffle\n","random.seed(1)\n","import json\n","\n","\n","# stop words list\n","stop_words = []\n","with open(\"/content/vietnamese-stopwords/vietnamese-stopwords.txt\", \"r\") as f:\n","    stop_words = []\n","    for line in f:\n","        dd = line.strip('\\n')\n","        stop_words.append(dd)\n","\n","# cleaning up text\n","import re\n","\n","\n","def get_only_chars(line):\n","    return line\n","\n","\n","########################################################################\n","# Synonym replacement\n","# Replace n words in the sentence with synonyms from wordnet\n","########################################################################\n","\n","\n","from nltk.corpus import wordnet\n","\n","\n","def synonym_replacement(words, n):\n","    new_words = words.copy()\n","    random_word_list = list(set([word for word in words if word not in stop_words]))\n","    random.shuffle(random_word_list)\n","    num_replaced = 0\n","    for random_word in random_word_list:\n","        synonyms = get_synonyms(random_word)\n","        if len(synonyms) >= 1:\n","            synonym = random.choice(list(synonyms))\n","            new_words = [synonym if word == random_word else word for word in new_words]\n","            # print(\"replaced\", random_word, \"with\", synonym)\n","            num_replaced += 1\n","        if num_replaced >= n:  # only replace up to n words\n","            break\n","\n","    # this is stupid but we need it, trust me\n","    sentence = ' '.join(new_words)\n","    new_words = sentence.split(' ')\n","\n","    return new_words\n","\n","\n","\n","def get_synonyms(word):\n","    synonyms = set()\n","    with open(\"/content/text_augmentation_vietnamese/word_net_vi.json\", \"r\") as f:\n","        wordnet = json.load(f)\n","\n","    for key, value in wordnet.items():\n","        if key.strip() == word:\n","            for v in value:\n","                synonyms.add(v.strip())\n","\n","        if word in synonyms:\n","            synonyms.remove(word)\n","    return list(synonyms)\n","\n","\n","########################################################################\n","# Random deletion\n","# Randomly delete words from the sentence with probability p\n","########################################################################\n","\n","def random_deletion(words, p):\n","    # obviously, if there's only one word, don't delete it\n","    if len(words) == 1:\n","        return words\n","\n","    # randomly delete words with probability p\n","    new_words = []\n","    for word in words:\n","        r = random.uniform(0, 1)\n","        if r > p:\n","            new_words.append(word)\n","\n","    # if you end up deleting all words, just return a random word\n","    if len(new_words) == 0:\n","        rand_int = random.randint(0, len(words) - 1)\n","        return [words[rand_int]]\n","\n","    return new_words\n","\n","\n","########################################################################\n","# Random swap\n","# Randomly swap two words in the sentence n times\n","########################################################################\n","\n","def random_swap(words, n):\n","    new_words = words.copy()\n","    for _ in range(n):\n","        if len(new_words) > 0:\n","            new_words = swap_word(new_words)\n","    return new_words\n","\n","\n","def swap_word(new_words):\n","    random_idx_1 = random.randint(0, len(new_words) - 1)\n","    random_idx_2 = random_idx_1\n","    counter = 0\n","    while random_idx_2 == random_idx_1:\n","        random_idx_2 = random.randint(0, len(new_words) - 1)\n","        counter += 1\n","        if counter > 3:\n","            return new_words\n","    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n","    return new_words\n","\n","\n","########################################################################\n","# Random insertion\n","# Randomly insert n words into the sentence\n","########################################################################\n","\n","def random_insertion(words, n):\n","    new_words = words.copy()\n","    for _ in range(n):\n","        add_word(new_words)\n","    return new_words\n","\n","\n","def add_word(new_words):\n","    synonyms = []\n","    counter = 0\n","    while len(synonyms) < 1 and len(new_words) > 0:\n","    # while len(synonyms) < 1:\n","        random_word = new_words[random.randint(0, len(new_words) - 1)]\n","        synonyms = get_synonyms(random_word)\n","        counter += 1\n","        if counter >= 10:\n","            return\n","    \n","    if len(new_words) > 0:\n","        random_synonym = synonyms[0]\n","        random_idx = random.randint(0, len(new_words) - 1)\n","        new_words.insert(random_idx, random_synonym)\n","\n","\n","########################################################################\n","# main data augmentation function\n","########################################################################\n","\n","def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n","    sentence = get_only_chars(sentence)\n","    words = sentence.split(' ')\n","    words = [word for word in words if word is not '']\n","    num_words = len(words)\n","\n","    augmented_sentences = []\n","\n","    if len(words) <= 0:\n","        return augmented_sentences\n","    num_new_per_technique = int(num_aug / 4) + 1\n","    n_sr = max(1, int(alpha_sr * num_words))\n","    n_ri = max(1, int(alpha_ri * num_words))\n","    n_rs = max(1, int(alpha_rs * num_words))\n","\n","    # sr\n","    for _ in range(num_new_per_technique):\n","        a_words = synonym_replacement(words, n_sr)\n","        augmented_sentences.append(' '.join(a_words))\n","\n","    # ri\n","    for _ in range(num_new_per_technique):\n","        a_words = random_insertion(words, n_ri)\n","        augmented_sentences.append(' '.join(a_words))\n","\n","    # rs\n","    for _ in range(num_new_per_technique):\n","        a_words = random_swap(words, n_rs)\n","        augmented_sentences.append(' '.join(a_words))\n","\n","    # rd\n","    for _ in range(num_new_per_technique):\n","        a_words = random_deletion(words, p_rd)\n","        augmented_sentences.append(' '.join(a_words))\n","\n","    augmented_sentences = list(set(augmented_sentences))\n","    augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n","    shuffle(augmented_sentences)\n","\n","    # trim so that we have the desired number of augmented sentences\n","    if num_aug >= 1:\n","        augmented_sentences = augmented_sentences[:num_aug]\n","    else:\n","        keep_prob = num_aug / len(augmented_sentences)\n","        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n","\n","    # append the original sentence\n","    augmented_sentences.append(sentence)\n","\n","    return augmented_sentences\n","\n","\n","# the output file\n","output = None\n","if args.output:\n","    output = args.output\n","else:\n","    from os.path import dirname, basename, join\n","\n","    output = join(dirname(args.input), 'eda_' + basename(args.input))\n","\n","# number of augmented sentences to generate per original sentence\n","num_aug = 4  # default\n","if args.num_aug:\n","    num_aug = args.num_aug\n","\n","# how much to change each sentence\n","alpha = 0.1  # default\n","if args.alpha:\n","    alpha = args.alpha\n","\n","\n","# generate more data with standard augmentation\n","def gen_eda(train_orig, output_file, alpha, num_aug=9):\n","    try:\n","        writer = open(output_file, 'w')\n","        lines = open(train_orig, 'r', encoding=\"utf-8\").readlines()\n","\n","        writer.write(\"free_text\" + \",\" + \"label_id\" + '\\n')\n","        augm = \"\"\n","        for i, line in enumerate(lines):\n","            try:\n","                parts = line[:-1].split('|')\n","                # print(parts)\n","                # sen_id = parts[0]\n","                label = parts[1]\n","                sentence = parts[0]\n","                aug_sentences = eda(sentence, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=num_aug)\n","                for aug_sentence in aug_sentences:\n","                    augm = augm + aug_sentence + \",\" + label + '\\n'\n","            except Exception as e:\n","                print(e)\n","                print(parts)\n","                pass\n","\n","        writer.write(augm)\n","        writer.close()\n","        print(\n","            \"generated augmented sentences with eda for \" + train_orig + \" to \" + output_file + \" with num_aug=\" + str(num_aug))\n","    except Exception as e:\n","        raise e\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ze-zSNqdL2xN"},"outputs":[],"source":["# main function. Run this cell to generate new data \n","if __name__ == \"__main__\":\n","    # generate augmented sentences and output into a new file\n","    gen_eda(args.input, args.output, alpha=alpha, num_aug=num_aug)"]},{"cell_type":"markdown","source":["### Kiểm tra dữ liệu được tạo ra"],"metadata":{"id":"QeWLz7kpbkEP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"32B0sSEzBpKi"},"outputs":[],"source":["data_aug = pd.read_csv('/content/aug.txt', index_col=False, error_bad_lines=False)\n","data_aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zK6k-8Tb5DxQ"},"outputs":[],"source":["data_aug.drop_duplicates(subset =\"free_text\", keep = False, inplace = True)"]},{"cell_type":"code","source":["data_aug = pd.read_csv('/content/drive/MyDrive/Project_HK1_2022/NLP/data/Dữ liệu tự thu thập và gán nhãn/Generated_data_by_code/aug_data.csv')"],"metadata":{"id":"ERBRGY_JW5g-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_aug.drop('Unnamed: 0', inplace = True, axis = 1)"],"metadata":{"id":"3SuoIguh6ojr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0w1CTbj5HAo"},"outputs":[],"source":["df1 = pd.read_csv(FD_PATH, encoding=\"utf-8\", header=None, sep='|')\n","df1.head()"]},{"cell_type":"code","source":["df1.isnull().sum()"],"metadata":{"id":"zza6JgCxmRYG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hy86BLcv6IUh"},"outputs":[],"source":["df1.rename(columns = {0: 'free_text', 1:'label_id'}, inplace = True)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NezEZh5k6X9I"},"outputs":[],"source":["df1.head()"]},{"cell_type":"markdown","source":["### Gộp lại với bộ dữ liệu cũ"],"metadata":{"id":"xLXWNaoENuSH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WtHmRwBr5nTr"},"outputs":[],"source":["data_aug_and_ori = pd.concat([df1, data_aug])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":291,"status":"ok","timestamp":1671714078316,"user":{"displayName":"Quá Nguyễn Thanh Thiện","userId":"00835970338124974152"},"user_tz":-420},"id":"nxMHavcWB9vT","outputId":"2584127f-3e73-46a0-8544-4bf2a7a127de"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                           free_text          label_id  len1\n","0  ﻿nhân ae thế giới di động trần vẻ thời cà mau ...  SER&ACC#POSITIVE   117\n","1  ﻿nhân viên thế giới di động trần văn thời cà m...  SER&ACC#POSITIVE   117\n","2  ﻿nhân viên thế giới di động trần văn thời cà m...  SER&ACC#POSITIVE   112\n","3  ﻿nhân viên thế giới di động trần thời cà mau n...  SER&ACC#POSITIVE   110\n","4  ﻿nhân viên thế giới di động trần văn thời cà m...  SER&ACC#POSITIVE   117"],"text/html":["\n","  <div id=\"df-257b8be0-5135-438c-9afa-81f47060df15\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>free_text</th>\n","      <th>label_id</th>\n","      <th>len1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>﻿nhân ae thế giới di động trần vẻ thời cà mau ...</td>\n","      <td>SER&amp;ACC#POSITIVE</td>\n","      <td>117</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>﻿nhân viên thế giới di động trần văn thời cà m...</td>\n","      <td>SER&amp;ACC#POSITIVE</td>\n","      <td>117</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>﻿nhân viên thế giới di động trần văn thời cà m...</td>\n","      <td>SER&amp;ACC#POSITIVE</td>\n","      <td>112</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>﻿nhân viên thế giới di động trần thời cà mau n...</td>\n","      <td>SER&amp;ACC#POSITIVE</td>\n","      <td>110</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>﻿nhân viên thế giới di động trần văn thời cà m...</td>\n","      <td>SER&amp;ACC#POSITIVE</td>\n","      <td>117</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-257b8be0-5135-438c-9afa-81f47060df15')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-257b8be0-5135-438c-9afa-81f47060df15 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-257b8be0-5135-438c-9afa-81f47060df15');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}],"source":["data_aug['len1'] = data_aug['free_text'].apply(lambda x: len(x))\n","data_aug.head()"]},{"cell_type":"markdown","metadata":{"id":"3rOdt_RazskU"},"source":["### Convert to Jsonl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVlb0M_zsY1S"},"outputs":[],"source":["df_convert = []\n","for indx in data_aug.index:\n","  sentence ={}\n","  labels = []\n","  sentence[\"text\"]=data_aug.iloc[indx][\"free_text\"]\n","  labels.append([0, data_aug.iloc[indx]['len1'], data_aug.iloc[indx]['label_id']])\n","  sentence[\"labels\"]=labels\n","  #print(sentence)\n","  df_convert.append(sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsbwc59x2bVi"},"outputs":[],"source":["with open('/content/Ser_Cam_Des.jsonl',\"w\") as f:\n","  for i in df_convert:\n","    f.write(\"{}\\n\".format(i))\n","f.close()"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["BmlhP5t39Khb","Gl-wg2ZQb5Fu","QeWLz7kpbkEP","xLXWNaoENuSH","3rOdt_RazskU"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}